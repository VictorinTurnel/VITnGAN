%-------------------------------------
% ViTGAN Poster - Corrected for Overleaf Gemini Theme
%-------------------------------------

\documentclass[final]{beamer}

% ====================
% Packages
% ====================

\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage[orientation=portrait,size=a0,scale=1.0]{beamerposter}
\usetheme{gemini}
\usecolortheme{nott} % Assure-toi que le fichier beamercolorthemenott.sty est prÃ©sent, sinon change pour 'mit' ou 'gemini'
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.14}
\usepackage{anyfontsize}
\usepackage{xcolor}

% ----------------------------------
% TikZ Styles
% ----------------------------------
\usetikzlibrary{shapes.geometric, arrows, positioning}
\tikzstyle{process} = [rectangle, rounded corners, minimum width=3cm, minimum height=1cm, text centered, draw=black, fill=white]
\tikzstyle{arrow} = [thick,->,>=stealth]

% ====================
% Lengths
% ====================
\newlength{\sepwidth}
\newlength{\colwidth}
\setlength{\sepwidth}{0.025\paperwidth}
\setlength{\colwidth}{0.45\paperwidth}

\newcommand{\separatorcolumn}{\begin{column}{\sepwidth}\end{column}}

% ====================
% Title
% ====================

\title{ViTGAN: Training GANs with Vision Transformers}

\author{Kwonjoon Lee\inst{1,3} \and Huiwen Chang\inst{2} \and Lu Jiang\inst{2} \and Han Zhang\inst{2} \and Zhuowen Tu\inst{1} \and Ce Liu\inst{4}}

\institute[shortinst]{\inst{1} UC San Diego \samelineand \inst{2} Google Research \samelineand \inst{3} Honda Research Institute \samelineand \inst{4} Microsoft Azure AI}

% ====================
% Footer
% ====================
\footercontent{
  \textbf{ICLR 2022} \hfill
  \textbf{Paper: arXiv:2107.04589} \hfill
  \href{mailto:kwl042@eng.ucsd.edu}{\textbf{Contact Author}}}

% ====================
% Body
% ====================

\begin{document}

\begin{frame}[t]
\begin{columns}[t]
\separatorcolumn

% ====================
% COLUMN 1
% ====================
\begin{column}{\colwidth}

% ----------------------------------
% Abstract
% ----------------------------------
  \begin{block}{Abstract}
    Recently, Vision Transformers (ViTs) have shown competitive performance on image recognition. In this work, we investigate if such performance can be extended to \textbf{image generation} (GANs) without using convolutions. We identify that standard ViTs cause serious instability in GAN training. We introduce \textbf{ViTGAN}, which integrates novel regularization techniques for the discriminator and architectural choices for the generator, achieving performance comparable to state-of-the-art CNN-based GANs (StyleGAN2).
  \end{block}
  
% ----------------------------------
% Introduction
% ----------------------------------
  \begin{block}{Introduction \& Motivation}
    Convolutional Neural Networks (CNNs) dominate computer vision due to inductive biases. However, Vision Transformers (ViTs) capture long-range dependencies via self-attention.
    
    \vspace{0.5em}
    \textbf{The Gap:} While ViTs excel at classification, using them for \textbf{generative adversarial training} is challenging due to severe instability.
    
    \vspace{0.5em}
    \textbf{Research Question:} Can we build a GAN entirely out of Vision Transformers (without convolutions) that matches the quality of CNN-based GANs?
  \end{block}

% ----------------------------------
% Challenges
% ----------------------------------
  \begin{alertblock}{The Challenge: Training Instability}
    Standard regularization techniques (Gradient Penalty, Spectral Norm) used in CNN-GANs fail with ViTs.
    
    \begin{itemize}
        \item \textbf{Lipschitz Continuity:} Standard dot-product self-attention can have an unbounded Lipschitz constant, violating GAN discriminator requirements.
        \item \textbf{Gradient Variance:} Adversarial training is hindered by high-variance gradients in later stages.
    \end{itemize}
  \end{alertblock}

% ----------------------------------
% Methodology: Generator
% ----------------------------------
 \begin{block}{Method: ViTGAN Generator}
    The generator maps a latent noise vector $z$ to a sequence of image patches.
    
    \textbf{1. Self-Modulated LayerNorm (SLN)}:
    Instead of feeding $z$ as input tokens, we use $z$ to \textbf{modulate} the normalization layers:
    $$ \text{SLN}(h_l, w) = \gamma_l(w) \odot \frac{h_l - \mu}{\sigma} + \beta_l(w), \quad w = \text{MLP}(z) $$
    
    \textbf{2. Implicit Neural Representation}:
    To generate pixels from patch embeddings, we use Fourier-feature based implicit representation ($E_{fou}$) enhancing spatial smoothness.

    % --- TikZ Diagram ---
    \vspace{1cm}
    \begin{center}
    \begin{tikzpicture}[node distance=2cm]
        \node (noise) [process, fill=blue!10] {Latent $z$};
        \node (map) [process, right of=noise, xshift=4cm] {Mapping Network};
        \node (trans) [process, below of=map, fill=green!10, text width=8cm] {Transformer Encoder (SLN)};
        \node (patch) [process, below of=trans, fill=orange!10] {Implicit Representation};
        \node (img) [process, below of=patch, fill=red!10] {Generated Image};

        \draw [arrow] (noise) -- (map);
        \draw [arrow] (map) -- node[right] {$w$} (trans);
        \draw [arrow] (trans) -- (patch);
        \draw [arrow] (patch) -- (img);
    \end{tikzpicture}
    \end{center}
 \end{block}

\end{column}

\separatorcolumn

% ====================
% COLUMN 2
% ====================
\begin{column}{\colwidth}

% ----------------------------------
% Methodology: Discriminator
% ----------------------------------
\begin{block}{Method: ViTGAN Discriminator}
    To enforce stability and Lipschitz continuity:
    
    \begin{itemize}
        \item \textbf{L2 Attention:} Replaces dot-product similarity with Euclidean distance.
        $$ \text{Attention}(X) = \text{softmax}\left(-\frac{d(XW_q, XW_k)}{\sqrt{d_h}}\right) $$
        \item \textbf{Improved Spectral Normalization (ISN):} Enforced at initialization to prevent rank-1 collapse.
        \item \textbf{Overlapping Patches:} Allows overlap to preserve local information.
    \end{itemize}
\end{block}

% ----------------------------------
% Experiments
% ----------------------------------
\begin{block}{Experiments \& Results}
    \textbf{Stability Analysis (CIFAR-10):}
    Our techniques stabilize training where baselines fail.
    
    \vspace{0.5em}
    \begin{table}
    \centering
    \begin{tabular}{l c c}
    \toprule
    \textbf{Regularization Method} & \textbf{FID} ($\downarrow$) & \textbf{IS} ($\uparrow$) \\
    \midrule
    Baseline + R1 Penalty & NaN (Fail) & NaN \\
    Baseline + Spectral Norm & 10.2 & 8.78 \\
    $L_2$ + SN & 16.8 & 2.36 \\
    \textbf{$L_2$ + ISN + Overlap (Ours)} & \textbf{4.92} & \textbf{9.69} \\
    \bottomrule
    \end{tabular}
    \caption{Ablation study of regularization techniques.}
    \end{table}

    \vspace{1em}
    \textbf{Comparison with SOTA:}
    ViTGAN achieves FID of \textbf{4.92} on CIFAR-10, comparable to StyleGAN2 (FID 5.60).
\end{block}

% ----------------------------------
% Visual Results Placeholder
% ----------------------------------
   \begin{block}{Visual Results}
    \begin{center}
        % Remplacer ce bloc TikZ par \includegraphics quand vous aurez l'image
        \begin{tikzpicture}
            \draw[fill=gray!20, draw=gray!50, dashed] (0,0) rectangle (15,8);
            \node at (7.5,4) {\Huge \textcolor{gray}{Insert Figure 3 Here}};
            \node at (7.5,2.5) {\large (Generated Samples)};
        \end{tikzpicture}
    \end{center}
    
    Generated images show high fidelity and coherent structures, avoiding typical CNN texture artifacts.
  \end{block}

% ----------------------------------
% Conclusion
% ----------------------------------
  \begin{exampleblock}{Conclusion}
    \begin{itemize}
      \item We demonstrated that \textbf{purely Transformer-based GANs} are viable.
      \item Key contributions (L2 Attention, ISN, SLN) effectively solve instability.
      \item ViTGAN closes the performance gap with CNN-based models like StyleGAN2.
    \end{itemize}
  \end{exampleblock}

% ----------------------------------
% References
% ----------------------------------
  \begin{block}{References}
    \footnotesize
    \begin{thebibliography}{10}
    \bibitem{lee2022} Lee, K. et al. (2022). ViTGAN: Training GANs with Vision Transformers. \textit{ICLR}.
    \bibitem{dosovitskiy2021} Dosovitskiy, A. et al. (2021). An Image is Worth 16x16 Words. \textit{ICLR}.
    \end{thebibliography}
  \end{block}

\end{column}
\separatorcolumn

\end{columns}
\end{frame}

\end{document}